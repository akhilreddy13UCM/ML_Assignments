{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Applied PCA on CC General data set\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Droped the ID column and categorical columns if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Filled any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scaled the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Initialized PCA model with 2 components\npca = PCA(n_components=2)\n\n# Fit and transform the data using PCA\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Printing the explained variance ratio\nprint('Explained variance ratio:', pca.explained_variance_ratio_)\n\n# Created a new dataframe with the transformed data\ncc_pca_df = pd.DataFrame(data=cc_pca, columns=['PC1', 'PC2'])\n\n# Printing the transformed data\nprint('Transformed data:', cc_pca_df.head())\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": "Explained variance ratio: [0.28845814 0.21570572]\nTransformed data:         PC1       PC2\n0 -1.718892 -1.072939\n1 -1.169309  2.509314\n2  0.938421 -0.382581\n3 -0.907504  0.045853\n4 -1.637828 -0.684971\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Calculate silhouette score without applying pca\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Initialize k-means model with 2 clusters\nkmeans = KMeans(n_clusters=2)\n\n# Fit the k-means model on the scaled data\nkmeans.fit(cc_scaled)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_scaled, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "text": "Silhouette score: 0.22588997653013274\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Calculate silhouette score applying pca\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Initialize PCA model with 2 components\npca = PCA(n_components=2)\n\n# Fit and transform the data using PCA\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Initialize k-means model with 2 clusters\nkmeans = KMeans(n_clusters=2)\n\n# Fit the k-means model on the PCA transformed data\nkmeans.fit(cc_pca)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_pca, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": "Silhouette score: 0.464435841376814\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Perform Scaling+PCA+K-Means and report performance with 2 clusters in kmeans\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Apply PCA\npca = PCA(n_components=2)\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Initialize k-means model with 2 clusters\nkmeans = KMeans(n_clusters=2)\n\n# Fit the k-means model on the PCA data\nkmeans.fit(cc_pca)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_pca, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": "Silhouette score: 0.4628339963096451\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "#Perform Scaling+PCA+K-Means and report performance with 3 clusters in kmeans\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load the dataset\ncc_data = pd.read_csv('CC GENERAL.csv')\n\n# Drop the categorical columns and ID column if present\ncc_data = cc_data.drop(['CUST_ID', 'TENURE'], axis=1)\n\n# Fill any missing values with mean of respective column\ncc_data = cc_data.fillna(cc_data.mean())\n\n# Scale the data using StandardScaler\nscaler = StandardScaler()\ncc_scaled = scaler.fit_transform(cc_data)\n\n# Apply PCA\npca = PCA(n_components=2)\ncc_pca = pca.fit_transform(cc_scaled)\n\n# Initialize k-means model with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=42)\n\n# Fit the k-means model on the PCA data\nkmeans.fit(cc_pca)\n\n# Calculate the silhouette score of the clustered data\nsilhouette_avg = silhouette_score(cc_pca, kmeans.labels_)\n\n# Print the silhouette score\nprint('Silhouette score:', silhouette_avg)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": "Silhouette score: 0.45332046852451596\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}